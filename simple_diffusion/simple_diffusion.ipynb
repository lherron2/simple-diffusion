{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7410dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "822b77a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from types import SimpleNamespace\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from loader import TorsionLoader\n",
    "from Unet1D import Unet1D\n",
    "from backbone import ConvBackbone1D\n",
    "from diffusion import VPDiffusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dae049f",
   "metadata": {},
   "source": [
    "<font size=\"3\">The directory contains information about paths and naming conventions.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06496dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = SimpleNamespace(model_path=\"../saved_models/\",\n",
    "                            data_path=\"../data/aib9.npy\",\n",
    "                            sample_path=\"../samples/\",\n",
    "                            identifier=\"aib9\"\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4191356",
   "metadata": {},
   "source": [
    "<font size=\"3\">Here we initialize our __TorsionLoader__ object (torsion angles of the Aib9 peptide) and our deep learning architecture which will parameterize the score (a __1D Unet__).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c31a340b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The torsion data has shape (500000, 1, 18).\n",
      "There are 18 torsion angles and 8 resnet block groups,\n",
      "so the model should have 24 dimensions\n"
     ]
    }
   ],
   "source": [
    "loader = TorsionLoader(directory.data_path)\n",
    "\n",
    "resnet_block_groups = 8 # model size\n",
    "num_torsions = loader.__getitem__(0).shape[-1]\n",
    "model_dim = int(np.ceil(num_torsions/resnet_block_groups) * resnet_block_groups)\n",
    "\n",
    "print(f\"The torsion data has shape {tuple(loader.data.shape)}.\")\n",
    "print(f\"There are {num_torsions} torsion angles and {resnet_block_groups} resnet block groups,\")\n",
    "print(f\"so the model should have {model_dim} dimensions\")\n",
    "\n",
    "model = Unet1D(dim=model_dim,\n",
    "               channels=1,\n",
    "               resnet_block_groups=resnet_block_groups,\n",
    "               learned_sinusoidal_cond=True,\n",
    "               learned_sinusoidal_dim=16\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34b6214",
   "metadata": {},
   "source": [
    "<font size=\"3\">The __Backbone__ class is simply a wrapper around the architecture used to predict the score. It automatically initializes optimizers and learning rate schedulers, and has methods to save and load models from files. It also handles upsampling and downsampling of data to be compatible with the model.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88e58c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = ConvBackbone1D(model=model, # model\n",
    "                          data_shape=num_torsions, # data shape\n",
    "                          target_shape=model_dim, # network shape\n",
    "                          num_dims=len(loader.data.shape),\n",
    "                          lr=1e-4\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe462f9",
   "metadata": {},
   "source": [
    "<font size=\"3\">The __VPDiffusion__ class implements the diffusion process in __[Score-Based Generative Modeling through Stochastic Differential Equations](https://openreview.net/pdf/ef0eadbe07115b0853e964f17aa09d811cd490f1.pdf)__. In the paper, the authors derive three SDEs with gaussian stationary distributions: the Variance Exploding (VE) SDE, the Variance Preserving (VP) SDE, and the sub-Variance preserving (sub-VP) SDE. \n",
    "</font>\n",
    "\n",
    "<font size=\"3\">Each SDE differs in their choice of drift and and noise coefficients.\n",
    "</font>\n",
    "\n",
    "<font size=\"3\">The VP-SDE has the form\n",
    "</font>\n",
    "\n",
    "<font size=\"3\">\n",
    "$$\n",
    "\\mathrm{d}\\mathbf{x} = -\\frac{1}{2}\\beta(t) \\mathbf{x}\\mathrm{d}t + \\sqrt{\\beta(t)}d\\mathbf{w}.\n",
    "$$\n",
    "</font>\n",
    "<br>\n",
    "<font size=\"3\">where $\\beta(t)$ is a monotonically increasing function of $t$. Since the drift coefficient $f(x,t) = \\frac{1}{2}\\beta(t)\\mathbf{x}$ is linear in $\\mathbf{x}$, the transition kernels have a simple closed-form solutions:\n",
    "</font>\n",
    "\n",
    "<font size=\"3\">$$\n",
    "\\mathcal{N}\\left(\\mathbf{x}_t | \\mathbf{x}_0\\right) = \\mathcal{N}\\left(e^{-\\frac{1}{2}\\int_0^t \\beta(s)ds}\\mathbf{x}_0 \\; \\Big|\\;(1-e^{-\\frac{1}{2}\\int_0^t \\beta(s)ds}) \\mathbf{I}\\right).\n",
    "$$\n",
    "</font>\n",
    "<br>\n",
    "<font size=\"3\">It is often easier to work with $\\alpha_t = e^{-\\frac{1}{2}\\int_0^t \\beta(s)ds}$ directly rather than $\\beta(t)$, which is the approach we will take here. We use the polynomial noise parameterization in Hoogeboom et. al.\n",
    "</font>\n",
    "<font size=\"3\">The __VPDiffusion__ class has methods which apply the transition kernels $p(x_t|x_0)$, $q(x_0|x_t)$, and $p(x_{t+1}|x_t)$ to the data. These transition kernels characterize the discrete-time forward ($p$) and reverse ($q$) SDEs. This is the Markov chain formulation originally given in __[Deep Unsupervised Learning using Nonequilibrium Thermodynamics](http://proceedings.mlr.press/v37/sohl-dickstein15.pdf), [Denoising Diffusion Probabilistic Models](https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf)__, and __[Denoising Diffusion Implicit Models](https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf)__.\n",
    "</font>\n",
    "\n",
    "<font size=\"3\">The discrete (or continuous) time forward SDE can be evaluated in a single step as\n",
    "    $$\n",
    "    \\mathbf{x}_t = \\sqrt{\\alpha_{t}}\\mathbf{x}_0 + \\sqrt{1-\\alpha_t}\\epsilon\n",
    "    $$\n",
    "    since the above kernel has closed form. Note that the original sample can be reconstructed given $\\mathbf{x}_t$ and $\\epsilon$ by solving for $\\mathbf{x}_0$ (this reconstruction is typically denoted as $\\mathbf{\\hat{x}}^0_t$, and is important for inference when the true $\\mathbf{x}_0$ does not exist or is unknown.\n",
    "</font>\n",
    "\n",
    "<font size=\"3\">Less straightforwardly, the reverse SDE can be discretized as\n",
    "    $$\\mathbf{x}_{t+1} = \\sqrt{\\alpha_{t-1}}\\left( \\frac{\\mathbf{x}_t - \\sqrt{1-\\alpha_t}\\mathbf{\\epsilon}_\\theta^{(t)}(\\mathbf{x}_t)}{\\sqrt{\\alpha_t}}\\right) + \\sqrt{1 âˆ’ \\alpha_{t-1} - \\sigma_t^2}\\epsilon_\\theta^{(t)}(\\mathbf{x}_t) + \\sigma_t \\epsilon_t,$$ where $\\epsilon_t$ is isotropic gaussian noise, and the first term inside the parentheses is $\\mathbf{\\hat{x}}_0^{(t)}$ &ndash; the network's prediction of $\\mathbf{x}_0$ conditional on the information present at timestep $t$. The second term points back towards the noisy sample $\\mathbf{x}_t$, so that the process of moving towards $\\mathbf{x}_0$ is gradual. The term $\\sigma_t$ can be chosen freeley. A choice of $\\sigma_t = \\sqrt{(1-\\alpha_{t-1})/({1-\\alpha_t})}\\sqrt{1-\\alpha_t/\\alpha_{t-1}}$ corresponds to the original DDPM implementation, while choosing $\\sigma_t = 0$ makes the generative process deterministic, turning the reverse-diffusion SDE into an ODE. \n",
    "</font>\n",
    "\n",
    "<font size=\"3\">The implementation of __VPDiffusion__ uses this update equation with $\\sigma_t=0$ to transform noise into data.\n",
    "</font>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "379d483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion = VPDiffusion(num_diffusion_timesteps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf391f",
   "metadata": {},
   "source": [
    "<font size=\"3\">Diffusion models can be trained with variational or score-matching objectives. In fact, the two loss functions only differ by a multiplicative factor relating to the parameterization of the noise schedule. The score-matching loss and simpler and more widely used in the community, so we will use it here. The score-matching loss is \n",
    "$$\n",
    "\\mathcal{L} =\\sum\\nolimits_{\\mathbb{E}(\\mathbf{x}_0 \\sim p(\\mathbf{x}_0), \\epsilon \\sim \\mathcal{N}(0, \\mathbf{I}))} \\lvert \\lvert \\epsilon_\\theta^{(t)}(\\mathbf{x}_t) - \\epsilon \\rvert \\rvert^2.\n",
    "$$\n",
    "And $\\epsilon_\\theta^{(t)}(\\mathbf{x}_t, \\alpha_t)$ is the prediction of the noise which was added to the original sample $\\mathbf{x}_0$ to produce $\\mathbf{x}_t$\n",
    "</font>\n",
    "\n",
    "\n",
    "<font size=\"3\">Oftentimes the network is trained on the score-matching loss, and the variational loss (see [__Variational Diffusion Models__](https://openreview.net/pdf?id=2LdBqxc1Yv)) is computed during sampling to find log-probabilities (free energies). So, training a diffusion model is simple. First randomly choose a sample $\\mathbf{x}_0$ and add randomly sampled noise $\\epsilon \\sim \\mathcal{N}(0, \\mathbf{I})$. Then compute $\\mathbf{x}_t$ using $\\mathbf{x}_0$, $\\epsilon$, and the noise schedule $\\alpha_t$. Finally, train a neural network to estimate $\\epsilon$ given $\\mathbf{x}_t$ using the objective above.  \n",
    "</font>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ebb0a3f9",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_loop(train_loader, backbone, diffusion, num_epochs=50):\n",
    "    \n",
    "    def l2_loss(x, x_pred):\n",
    "        return (x - x_pred).pow(2).sum((1,2)).pow(0.5).mean()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, b in enumerate(train_loader, 0):\n",
    "\n",
    "            # sample set of times\n",
    "            t = torch.randint(low=0, \n",
    "                          high=diffusion.num_diffusion_timesteps, \n",
    "                          size=(b.size(0),)).long()\n",
    "\n",
    "            # corrupt data according to noise schedule\n",
    "            b_t, e_0 = diffusion.forward_kernel(b, t)\n",
    "\n",
    "            # predict noise and original data\n",
    "            b_0, e_t = diffusion.reverse_kernel(b_t, t, backbone, \"x0\")\n",
    "\n",
    "            # evaluate loss and do backprop\n",
    "            loss = l2_loss(b_t, b_0)\n",
    "            backbone.optim.zero_grad()\n",
    "            loss.backward()\n",
    "            backbone.optim.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"step: {i}, loss {loss.detach():.3f}\")\n",
    "        backbone.save_state(directory, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26be1717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss 4.075\n",
      "step: 100, loss 0.451\n",
      "step: 200, loss 0.182\n",
      "step: 300, loss 0.113\n",
      "step: 400, loss 0.084\n",
      "step: 500, loss 0.070\n",
      "step: 600, loss 0.058\n",
      "step: 700, loss 0.055\n",
      "step: 800, loss 0.045\n",
      "step: 900, loss 0.040\n",
      "step: 0, loss 0.041\n",
      "step: 100, loss 0.038\n",
      "step: 200, loss 0.029\n",
      "step: 300, loss 0.030\n",
      "step: 400, loss 0.030\n",
      "step: 500, loss 0.025\n",
      "step: 600, loss 0.025\n",
      "step: 700, loss 0.022\n",
      "step: 800, loss 0.020\n",
      "step: 900, loss 0.023\n",
      "step: 0, loss 0.022\n",
      "step: 100, loss 0.021\n",
      "step: 200, loss 0.019\n",
      "step: 300, loss 0.020\n",
      "step: 400, loss 0.017\n",
      "step: 500, loss 0.018\n",
      "step: 600, loss 0.015\n",
      "step: 700, loss 0.016\n",
      "step: 800, loss 0.016\n",
      "step: 900, loss 0.016\n",
      "step: 0, loss 0.014\n",
      "step: 100, loss 0.017\n",
      "step: 200, loss 0.016\n",
      "step: 300, loss 0.014\n",
      "step: 400, loss 0.013\n",
      "step: 500, loss 0.022\n",
      "step: 600, loss 0.019\n",
      "step: 700, loss 0.022\n",
      "step: 800, loss 0.023\n",
      "step: 900, loss 0.023\n",
      "step: 0, loss 0.020\n",
      "step: 100, loss 0.020\n",
      "step: 200, loss 0.018\n",
      "step: 300, loss 0.019\n",
      "step: 400, loss 0.020\n",
      "step: 500, loss 0.019\n",
      "step: 600, loss 0.015\n",
      "step: 700, loss 0.017\n",
      "step: 800, loss 0.016\n",
      "step: 900, loss 0.010\n",
      "step: 0, loss 0.009\n",
      "step: 100, loss 0.010\n",
      "step: 200, loss 0.010\n",
      "step: 300, loss 0.009\n",
      "step: 400, loss 0.010\n",
      "step: 500, loss 0.009\n",
      "step: 600, loss 0.009\n",
      "step: 700, loss 0.010\n",
      "step: 800, loss 0.010\n",
      "step: 900, loss 0.008\n",
      "step: 0, loss 0.009\n",
      "step: 100, loss 0.009\n",
      "step: 200, loss 0.010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(loader, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# training the diffusion model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiffusion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 23\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(train_loader, backbone, diffusion, num_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m l2_loss(b_t, b_0)\n\u001b[1;32m     22\u001b[0m backbone\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m backbone\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/scratch.tiwary-prj/lherron/miniconda/envs/ML/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scratch.tiwary-prj/lherron/miniconda/envs/ML/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(loader, batch_size=512, shuffle=True)\n",
    "\n",
    "# training the diffusion model\n",
    "train_loop(train_loader, backbone, diffusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe3172f",
   "metadata": {},
   "source": [
    "<font size=\"3\">Sampling amounts to generating random noise and iteratively solving the update equation above using the trained network.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3aab466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.load_model(directory, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24fd89af",
   "metadata": {
    "code_folding": [
     0,
     26
    ]
   },
   "outputs": [],
   "source": [
    "def sample_batch(batch_size, loader, diffusion, backbone, pred_type=\"x0\"):\n",
    "    \n",
    "    def sample_prior(batch_size, shape):\n",
    "        \"Generates samples of gaussian noise\"\n",
    "        prior_sample =  torch.randn(batch_size, *shape[1:], dtype=torch.float)\n",
    "        return prior_sample\n",
    "\n",
    "    def get_adjacent_times(times):\n",
    "        \"\"\"\n",
    "        Pairs t with t+1 for all times in the time-discretization\n",
    "        of the diffusion process.\n",
    "        \"\"\"\n",
    "        times_next = torch.cat((torch.Tensor([0]).long(), times[:-1]))\n",
    "        return list(zip(reversed(times), reversed(times_next)))\n",
    "\n",
    "    xt = sample_prior(batch_size, loader.data.shape)\n",
    "    time_pairs = get_adjacent_times(diffusion.times)\n",
    "\n",
    "    for t, t_next in time_pairs:\n",
    "        print(int(t))\n",
    "        t = torch.Tensor.repeat(t, batch_size)\n",
    "        t_next = torch.Tensor.repeat(t_next, batch_size)\n",
    "        xt_next = diffusion.reverse_step(xt, t, t_next, backbone, pred_type=pred_type)\n",
    "        xt = xt_next\n",
    "    return xt\n",
    "\n",
    "def save_batch(batch, save_prefix, save_idx):\n",
    "    os.makedirs(directory.sample_path, exist_ok=True)\n",
    "    file = os.path.join(directory.sample_path, f\"{save_prefix}_idx={save_idx}.npz\")\n",
    "    np.savez_compressed(file, data=batch)\n",
    "\n",
    "def sample_loop(num_samples, batch_size, save_prefix, loader, diffusion, backbone):\n",
    "    \n",
    "    # computing number of runs required to produce num_samples with batch_size constraint\n",
    "    n_runs = max(num_samples//batch_size, 1)\n",
    "    if num_samples <= batch_size:\n",
    "        batch_size = num_samples\n",
    "        \n",
    "    # iteratively sampling and saving\n",
    "    with torch.no_grad():\n",
    "        for save_idx in range(n_runs):\n",
    "            x0 = sample_batch(batch_size, loader, diffusion, backbone)\n",
    "            save_batch(x0, save_prefix, save_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e705d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "98\n",
      "97\n",
      "96\n",
      "95\n",
      "94\n",
      "93\n",
      "92\n",
      "91\n",
      "90\n",
      "89\n",
      "88\n",
      "87\n",
      "86\n",
      "85\n",
      "84\n",
      "83\n",
      "82\n",
      "81\n",
      "80\n",
      "79\n",
      "78\n",
      "77\n",
      "76\n",
      "75\n",
      "74\n",
      "73\n",
      "72\n",
      "71\n",
      "70\n",
      "69\n",
      "68\n",
      "67\n",
      "66\n",
      "65\n",
      "64\n",
      "63\n",
      "62\n",
      "61\n",
      "60\n",
      "59\n",
      "58\n",
      "57\n",
      "56\n",
      "55\n",
      "54\n",
      "53\n",
      "52\n",
      "51\n",
      "50\n",
      "49\n",
      "48\n",
      "47\n",
      "46\n",
      "45\n",
      "44\n",
      "43\n",
      "42\n",
      "41\n",
      "40\n",
      "39\n",
      "38\n",
      "37\n",
      "36\n",
      "35\n",
      "34\n",
      "33\n",
      "32\n",
      "31\n",
      "30\n",
      "29\n",
      "28\n",
      "27\n",
      "26\n",
      "25\n",
      "24\n",
      "23\n",
      "22\n",
      "21\n",
      "20\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "99\n",
      "98\n",
      "97\n",
      "96\n",
      "95\n",
      "94\n",
      "93\n",
      "92\n",
      "91\n",
      "90\n",
      "89\n",
      "88\n",
      "87\n",
      "86\n",
      "85\n",
      "84\n",
      "83\n",
      "82\n",
      "81\n",
      "80\n",
      "79\n",
      "78\n",
      "77\n",
      "76\n",
      "75\n",
      "74\n",
      "73\n",
      "72\n",
      "71\n",
      "70\n",
      "69\n",
      "68\n",
      "67\n",
      "66\n",
      "65\n",
      "64\n",
      "63\n",
      "62\n",
      "61\n",
      "60\n",
      "59\n",
      "58\n",
      "57\n",
      "56\n",
      "55\n",
      "54\n",
      "53\n",
      "52\n",
      "51\n",
      "50\n",
      "49\n",
      "48\n",
      "47\n",
      "46\n",
      "45\n",
      "44\n",
      "43\n",
      "42\n",
      "41\n",
      "40\n",
      "39\n",
      "38\n",
      "37\n",
      "36\n",
      "35\n",
      "34\n",
      "33\n",
      "32\n",
      "31\n",
      "30\n",
      "29\n",
      "28\n",
      "27\n",
      "26\n",
      "25\n",
      "24\n",
      "23\n",
      "22\n",
      "21\n",
      "20\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "99\n",
      "98\n",
      "97\n",
      "96\n",
      "95\n",
      "94\n",
      "93\n",
      "92\n",
      "91\n",
      "90\n",
      "89\n",
      "88\n",
      "87\n",
      "86\n",
      "85\n",
      "84\n",
      "83\n",
      "82\n",
      "81\n",
      "80\n",
      "79\n",
      "78\n",
      "77\n",
      "76\n",
      "75\n",
      "74\n",
      "73\n",
      "72\n",
      "71\n",
      "70\n",
      "69\n",
      "68\n",
      "67\n",
      "66\n",
      "65\n",
      "64\n",
      "63\n",
      "62\n",
      "61\n",
      "60\n",
      "59\n",
      "58\n",
      "57\n",
      "56\n",
      "55\n",
      "54\n",
      "53\n",
      "52\n",
      "51\n",
      "50\n",
      "49\n",
      "48\n",
      "47\n",
      "46\n",
      "45\n",
      "44\n",
      "43\n",
      "42\n",
      "41\n",
      "40\n",
      "39\n",
      "38\n",
      "37\n",
      "36\n",
      "35\n",
      "34\n",
      "33\n",
      "32\n",
      "31\n",
      "30\n",
      "29\n",
      "28\n",
      "27\n",
      "26\n",
      "25\n",
      "24\n",
      "23\n",
      "22\n",
      "21\n",
      "20\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "99\n",
      "98\n",
      "97\n",
      "96\n",
      "95\n",
      "94\n",
      "93\n",
      "92\n",
      "91\n",
      "90\n",
      "89\n",
      "88\n",
      "87\n",
      "86\n",
      "85\n",
      "84\n",
      "83\n",
      "82\n",
      "81\n",
      "80\n",
      "79\n",
      "78\n",
      "77\n",
      "76\n",
      "75\n",
      "74\n",
      "73\n",
      "72\n",
      "71\n",
      "70\n",
      "69\n",
      "68\n",
      "67\n",
      "66\n",
      "65\n",
      "64\n",
      "63\n",
      "62\n",
      "61\n",
      "60\n",
      "59\n",
      "58\n",
      "57\n",
      "56\n",
      "55\n",
      "54\n",
      "53\n",
      "52\n",
      "51\n",
      "50\n",
      "49\n",
      "48\n",
      "47\n",
      "46\n",
      "45\n",
      "44\n",
      "43\n",
      "42\n",
      "41\n",
      "40\n",
      "39\n",
      "38\n",
      "37\n",
      "36\n",
      "35\n",
      "34\n",
      "33\n",
      "32\n",
      "31\n",
      "30\n",
      "29\n",
      "28\n",
      "27\n",
      "26\n",
      "25\n",
      "24\n",
      "23\n",
      "22\n",
      "21\n",
      "20\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "99\n",
      "98\n",
      "97\n",
      "96\n",
      "95\n",
      "94\n",
      "93\n",
      "92\n",
      "91\n",
      "90\n",
      "89\n",
      "88\n",
      "87\n",
      "86\n",
      "85\n",
      "84\n",
      "83\n",
      "82\n",
      "81\n",
      "80\n",
      "79\n",
      "78\n",
      "77\n",
      "76\n",
      "75\n",
      "74\n",
      "73\n",
      "72\n",
      "71\n",
      "70\n",
      "69\n",
      "68\n",
      "67\n",
      "66\n",
      "65\n",
      "64\n",
      "63\n",
      "62\n",
      "61\n",
      "60\n",
      "59\n",
      "58\n",
      "57\n",
      "56\n",
      "55\n",
      "54\n",
      "53\n",
      "52\n",
      "51\n",
      "50\n",
      "49\n",
      "48\n",
      "47\n",
      "46\n",
      "45\n",
      "44\n",
      "43\n",
      "42\n",
      "41\n",
      "40\n",
      "39\n",
      "38\n",
      "37\n",
      "36\n",
      "35\n",
      "34\n",
      "33\n",
      "32\n",
      "31\n",
      "30\n",
      "29\n",
      "28\n",
      "27\n",
      "26\n",
      "25\n",
      "24\n",
      "23\n",
      "22\n",
      "21\n",
      "20\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "sample_loop(50000, 10000, \"aib9\", loader, diffusion, backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8383c56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-ipykernel",
   "language": "python",
   "name": "ml-ipykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
